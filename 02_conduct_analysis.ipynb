{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import nltk\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from itertools import combinations, chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from scipy.stats import pearsonr, ks_2samp, spearmanr\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc('ps',fonttype = 42)\n",
    "plt.rc('pdf',fonttype = 42)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.use14corefonts'] = True\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"German\", \"French\", \"Italian\", \"Spanish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_inptut_path = \"data/processed/texts.p\"\n",
    "sentiment_dir = \"data/sentiment/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_pickle(dataframe_inptut_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp = sc.load(\"de_core_news_sm\")\n",
    "fr_nlp = sc.load(\"fr_core_news_sm\")\n",
    "es_nlp = sc.load(\"es_core_news_sm\")\n",
    "it_nlp = sc.load(\"it_core_news_sm\")\n",
    "\n",
    "nlp_to_use = {\n",
    "    \"German\": de_nlp,\n",
    "    \"French\": fr_nlp,\n",
    "    \"Spanish\": es_nlp,\n",
    "    \"Italian\": it_nlp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexica = {}\n",
    "for lang in languages:\n",
    "    sentiment_lexica[lang] = {}\n",
    "    with open(\"{}negative_words_{}.txt\".format(sentiment_dir, lang.lower()), \"r\") as fr:\n",
    "        sentiment_lexica[lang][\"neg\"] = fr.read().splitlines()\n",
    "    with open(\"{}positive_words_{}.txt\".format(sentiment_dir, lang.lower()), \"r\") as fr:\n",
    "        sentiment_lexica[lang][\"pos\"] = fr.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_fix = {\n",
    "    \"Bachiller D. P. Gatell\": \"Bachiller D. P. Gatell.\",\n",
    "    \"Eliza Haywood\": \"Eliza Fowler Haywood\",\n",
    "}\n",
    "texts_df[\"author\"] = texts_df[\"author\"].replace(author_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"language\"] = texts_df[\"language\"].replace(\"Spanish; Castilian\", \"Spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\"-\")[0])\n",
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\" [\")[0])\n",
    "texts_df[\"date\"] = texts_df[\"date\"].apply(lambda x: x.split(\" bzw.\")[0])\n",
    "texts_df = texts_df[texts_df[\"date\"] != \"missing\"]\n",
    "texts_df = texts_df[texts_df[\"date\"] != \"4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce to defined languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df[texts_df[\"language\"].isin(languages)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    lang_df = texts_df.loc[texts_df[\"language\"] == language]\n",
    "    journal_group = lang_df.groupby(\"filename\")\n",
    "    authors = lang_df[\"author\"].unique()\n",
    "    num_authors = len(authors)\n",
    "    if \"Anonym\" in authors:\n",
    "        num_authors -= 1\n",
    "        num_anonymus = journal_group.apply(lambda x: 1 if all(x[\"author\"] == \"Anonym\") else 0).sum()\n",
    "    else:\n",
    "        num_anonymus = 0\n",
    "    if \"missing\" in authors:\n",
    "        num_authors -= 1\n",
    "        num_missing = journal_group.apply(lambda x: 1 if all(x[\"author\"] == \"missing\") else 0).sum()\n",
    "    else:\n",
    "        num_missing = 0\n",
    "    topics = lang_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "    years = lang_df[\"date\"].unique()\n",
    "        \n",
    "    print(\"num authors:\",  num_authors)\n",
    "    print(\"num_anonymous:\", num_anonymus)\n",
    "    print(\"num_missing:\", num_missing)\n",
    "    print(\"num journals:\", len(journal_group))\n",
    "    print(\"num text passages:\",  lang_df.shape[0])\n",
    "    print(\"num topics:\",  len(np.unique(topics)))\n",
    "    print(\"years:\", np.min(years), np.max(years))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = texts_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "topics_language_df = pd.merge(topics, texts_df[\"language\"].to_frame(), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = topics_language_df.groupby(\"language\")[\"value\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(row):\n",
    "    lang = row[\"language\"]\n",
    "    if lang not in nlp_to_use.keys():\n",
    "        return \"\"\n",
    "    doc = nlp_to_use[lang](row[\"text\"])\n",
    "    tokens = []\n",
    "    for t in doc:\n",
    "        #if t.pos_ == \"NOUN\":\n",
    "        tokens.append(t.lemma_)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_df[\"tokens\"] = texts_df.progress_apply(lemmatize, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze sentiment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, nl, pl):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    num_negative = 0\n",
    "    num_positive = 0\n",
    "    for nw in nl:\n",
    "        num_negative += tokens.count(nw.lower())\n",
    "    for pw in pl:\n",
    "        num_positive += tokens.count(pw.lower())\n",
    "    try:\n",
    "        score = (num_positive - num_negative) / (num_positive + num_negative)\n",
    "    except ZeroDivisionError:\n",
    "        score = \"ohne\"\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df[\"sentiment\"] = 0\n",
    "for language in languages:\n",
    "    lang_df = texts_df.loc[texts_df[\"language\"] == language]\n",
    "    neg_lexicon = sentiment_lexica[language][\"neg\"]\n",
    "    pos_lexicon = sentiment_lexica[language][\"pos\"]\n",
    "    scores = lang_df[\"text\"].progress_apply(analyze_sentiment, args=[neg_lexicon, pos_lexicon])\n",
    "    texts_df[\"sentiment\"].update(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graphs per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicity_thresholds = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(text):\n",
    "        temp_edges = []\n",
    "        for i in vertices:\n",
    "            if i in text:\n",
    "                temp_edges.append(i)\n",
    "        return list(combinations(sorted(temp_edges), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "connectivity_results = {}\n",
    "degree_results = {}\n",
    "path_results = {}\n",
    "cc_results = {}\n",
    "centrality_results_df = pd.DataFrame()\n",
    "assortativity_results = {}\n",
    "for language in languages:#[\"Italian\"]:\n",
    "    print(language)\n",
    "    \n",
    "    connectivity_results[language] = {}\n",
    "    degree_results[language] = {}\n",
    "    path_results[language] = {}\n",
    "    cc_results[language] = {}\n",
    "    assortativity_results[language] = {}\n",
    "    \n",
    "    text_by_filename_language_df = texts_df.groupby([\"filename\", \"language\", \"date\"])[\"tokens\"].apply(lambda x: \" \".join(x)).to_frame().reset_index()\n",
    "    language_df = text_by_filename_language_df[text_by_filename_language_df[\"language\"] == language]\n",
    "    stop_words = get_stop_words(language.lower())\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=1.0, min_df=1, token_pattern=\"[^\\d\\W]{3,}\")\n",
    "    _ = vectorizer.fit_transform(language_df[\"tokens\"])\n",
    "    vertices = vectorizer.get_feature_names()\n",
    "    neg_words = set()\n",
    "    pos_words = set()\n",
    "    for j in vertices:\n",
    "        if j in sentiment_lexica[language][\"neg\"]:\n",
    "            neg_words.add(j)\n",
    "        if j in sentiment_lexica[language][\"pos\"]:\n",
    "            pos_words.add(j)\n",
    "    vertices = neg_words.union(pos_words)\n",
    "    g = ig.Graph(directed=False)\n",
    "    for i in neg_words:\n",
    "        g.add_vertex(i, sent=1, color=\"red\", sentiment=\"neg\")\n",
    "    for i in pos_words:\n",
    "        g.add_vertex(i, sent=2, color=\"green\", sentiment=\"pos\")\n",
    "    edges = list(chain(*language_df[\"tokens\"].apply(get_edges)))\n",
    "    unique_edges, edge_counts = np.unique(edges, return_counts=True, axis=0)\n",
    "    g.add_edges(unique_edges)\n",
    "    g.es[\"multiplicity\"] = edge_counts\n",
    "    for mt in multiplicity_thresholds:\n",
    "        degree_results[language][mt] = {}\n",
    "        path_results[language][mt] = {}\n",
    "        cc_results[language][mt] = {}\n",
    "        assortativity_results[language][mt] = {}\n",
    "        g.delete_edges(np.where(np.array(g.es[\"multiplicity\"]) < mt)[0])\n",
    "        g.delete_vertices(np.where(np.array(g.degree()) == 0)[0])\n",
    "        graphs[language] = g\n",
    "        print(\"multiplicity_threshold:\", mt, \", num_vertices:\", g.vcount(), \", num_edges:\", g.ecount())\n",
    "        \n",
    "        # connectivity\n",
    "        print(\"num_components:\", len(g.clusters()))\n",
    "        k_cores = g.k_core()\n",
    "        num_components = []\n",
    "        for k, i in enumerate(k_cores, start=1):\n",
    "            num_components.append(len(i.clusters()))\n",
    "        connectivity_results[language][mt] = num_components\n",
    "        \n",
    "        # degree\n",
    "        degree_results[language][mt][\"all\"] = g.degree()\n",
    "        degree_results[language][mt][\"neg\"] = g.degree(np.where(np.array(g.vs[\"sentiment\"]) == \"neg\")[0])\n",
    "        degree_results[language][mt][\"pos\"] = g.degree(np.where(np.array(g.vs[\"sentiment\"]) == \"pos\")[0])\n",
    "        degree_results[language][mt][\"mean_degree\"] = np.mean(g.degree())\n",
    "        degree_results[language][mt][\"weighted_all\"] = g.strength(weights=\"multiplicity\")\n",
    "        degree_results[language][mt][\"weighted_neg\"] = g.strength(np.where(np.array(g.vs[\"sentiment\"]) == \"neg\")[0], weights=\"multiplicity\")\n",
    "        degree_results[language][mt][\"weighted_pos\"] = g.strength(np.where(np.array(g.vs[\"sentiment\"]) == \"pos\")[0], weights=\"multiplicity\")\n",
    "        degree_results[language][mt][\"mean_weighted_degree\"] = np.mean(g.strength(weights=\"multiplicity\"))\n",
    "        \n",
    "        # paths\n",
    "        path_results[language][mt][\"all\"] = g.eccentricity()\n",
    "        path_results[language][mt][\"neg\"] = g.eccentricity(np.where(np.array(g.vs[\"sentiment\"]) == \"neg\")[0])\n",
    "        path_results[language][mt][\"pos\"] = g.eccentricity(np.where(np.array(g.vs[\"sentiment\"]) == \"pos\")[0])\n",
    "        path_results[language][mt][\"average_path_length\"] = g.average_path_length(directed=False)\n",
    "        \n",
    "        # clustering coefficient\n",
    "        cc_results[language][mt][\"all\"] = g.transitivity_local_undirected()\n",
    "        cc_results[language][mt][\"neg\"] = g.transitivity_local_undirected(np.where(np.array(g.vs[\"sentiment\"]) == \"neg\")[0])\n",
    "        cc_results[language][mt][\"pos\"] = g.transitivity_local_undirected(np.where(np.array(g.vs[\"sentiment\"]) == \"pos\")[0])\n",
    "        cc_results[language][mt][\"weighted_all\"] = g.transitivity_local_undirected(weights=\"multiplicity\")\n",
    "        cc_results[language][mt][\"weighted_neg\"] = g.transitivity_local_undirected(np.where(np.array(g.vs[\"sentiment\"]) == \"neg\")[0], weights=\"multiplicity\")\n",
    "        cc_results[language][mt][\"weighted_pos\"] = g.transitivity_local_undirected(np.where(np.array(g.vs[\"sentiment\"]) == \"pos\")[0], weights=\"multiplicity\")\n",
    "        \n",
    "        # centralities\n",
    "        centrality_df = pd.DataFrame(np.array([g.vs[\"name\"], g.degree(), g.betweenness(directed=False), g.closeness(), g.strength(weights=\"multiplicity\"), g.betweenness(directed=False, weights=\"multiplicity\"), g.closeness(weights=\"multiplicity\")]).T, columns=[\"word\", \"degree\", \"betweenness\", \"closeness\", \"weighted_degree\", \"weighted_betweenness\", \"weighted_closeness\"])\n",
    "        centrality_df[\"language\"] = language\n",
    "        centrality_df[\"multiplicity_threshold\"] = mt\n",
    "        centrality_results_df = centrality_results_df.append(centrality_df)\n",
    "        \n",
    "        # assortativity\n",
    "        assortativity_results[language][mt][\"degree\"] = g.assortativity_degree(directed=False)\n",
    "        assortativity_results[language][mt][\"sentiment\"] = g.assortativity(\"sent\", directed=False)\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results/Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    for mt in multiplicity_thresholds:\n",
    "        print(max(connectivity_results[language][mt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    for mt in multiplicity_thresholds:\n",
    "        print(mt)\n",
    "        print(\"all\", min(path_results[language][mt][\"all\"]), max(path_results[language][mt][\"all\"]))\n",
    "        print(\"neg\", min(path_results[language][mt][\"neg\"]), max(path_results[language][mt][\"neg\"]))\n",
    "        print(\"pos\", min(path_results[language][mt][\"pos\"]), max(path_results[language][mt][\"pos\"]))\n",
    "        print(\"avp\", path_results[language][mt][\"average_path_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(15,3), sharey=True)\n",
    "for axidx, language in enumerate(languages):\n",
    "    degree_all = degree_results[language][1][\"all\"]\n",
    "    degree_neg = degree_results[language][1][\"neg\"]\n",
    "    degree_pos = degree_results[language][1][\"pos\"]\n",
    "        \n",
    "    degree_all_sorted = np.sort(degree_all)\n",
    "    degree_neg_sorted = np.sort(degree_neg)\n",
    "    degree_pos_sorted = np.sort(degree_pos)\n",
    "        \n",
    "    degree_all_p = np.linspace(0, 1, len(degree_all))\n",
    "    degree_neg_p = np.linspace(0, 1, len(degree_neg))\n",
    "    degree_pos_p = np.linspace(0, 1, len(degree_pos))\n",
    "        \n",
    "    sns.lineplot(degree_all_sorted, 1 - degree_all_p, color=\"black\", ci=None, ax=axs[axidx])\n",
    "    sns.lineplot(degree_neg_sorted, 1 - degree_neg_p, color=\"red\", ci=None, ax=axs[axidx])\n",
    "    sns.lineplot(degree_pos_sorted, 1 - degree_pos_p, color=\"green\", ci=None, ax=axs[axidx])\n",
    "    axs[axidx].set_xlabel(\"Degree\")\n",
    "    axs[axidx].set_title(language)\n",
    "axs[0].set_ylabel(\"CCDF\")\n",
    "axs[0].set_yticks([0, 0.5, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    degree_neg = degree_results[language][1][\"neg\"]\n",
    "    degree_pos = degree_results[language][1][\"pos\"]\n",
    "    weighted_degree_neg = degree_results[language][1][\"weighted_neg\"]\n",
    "    weighted_degree_pos = degree_results[language][1][\"weighted_pos\"]\n",
    "    \n",
    "    print(language, \"degree\", ks_2samp(degree_neg, degree_pos))\n",
    "    print(language, \"weighteddegree\", ks_2samp(weighted_degree_neg, weighted_degree_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCDF Eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10,4), sharey=True)\n",
    "for axidx, mt in enumerate(languages):\n",
    "    print(language, mt, path_results[language][mt][\"average_path_length\"])\n",
    "    path_all = path_results[language][mt][\"all\"]\n",
    "    path_neg = path_results[language][mt][\"neg\"]\n",
    "    path_pos = path_results[language][mt][\"pos\"]\n",
    "\n",
    "    path_all_sorted = np.sort(path_all)\n",
    "    path_neg_sorted = np.sort(path_neg)\n",
    "    path_pos_sorted = np.sort(path_pos)\n",
    "\n",
    "    path_all_p = np.linspace(0, 0.1, len(path_all))\n",
    "    path_neg_p = np.linspace(0, 1, len(path_neg))\n",
    "    path_pos_p = np.linspace(0, 1, len(path_pos))\n",
    "\n",
    "    sns.lineplot(path_all_sorted, 1 - path_all_p, color=\"black\", ci=None, ax=axs[axidx])\n",
    "    sns.lineplot(path_neg_sorted, 1 - path_neg_p, color=\"red\", ci=None, ax=axs[axidx])\n",
    "    sns.lineplot(path_pos_sorted, 1 - path_pos_p, color=\"green\", ci=None, ax=axs[axidx])\n",
    "    axs[axidx].set_xlabel(\"Eccentricity\")\n",
    "    axs[axidx].set_title(language)\n",
    "axs[0].set_ylabel(\"CCDF\")\n",
    "axs[0].set_yticks([0, 0.5, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCDF Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(15,3.3), sharey=True)\n",
    "for axidx, language in enumerate(languages):\n",
    "    cc_all = cc_results[language][1][\"weighted_all\"]\n",
    "    cc_neg = cc_results[language][1][\"weighted_neg\"]\n",
    "    cc_pos = cc_results[language][1][\"weighted_pos\"]\n",
    "\n",
    "    cc_all_sorted = np.sort(cc_all)\n",
    "    cc_neg_sorted = np.sort(cc_neg)\n",
    "    cc_pos_sorted = np.sort(cc_pos)\n",
    "\n",
    "    cc_all_p = np.linspace(0, 1, len(cc_all))\n",
    "    cc_neg_p = np.linspace(0, 1, len(cc_neg))\n",
    "    cc_pos_p = np.linspace(0, 1, len(cc_pos))\n",
    "        \n",
    "    sns.lineplot(cc_all_sorted, 1 - cc_all_p, color=\"black\", ci=None, ax=axs[axidx])\n",
    "    sns.lineplot(cc_neg_sorted, 1 - cc_neg_p, color=\"red\", ci=None, ax=axs[axidx])\n",
    "    sns.lineplot(cc_pos_sorted, 1 - cc_pos_p, color=\"green\", ci=None, ax=axs[axidx])\n",
    "    axs[axidx].set_xlabel(\"Weighted Local\\nClustering Coefficient\")\n",
    "    axs[axidx].set_title(language)\n",
    "axs[0].set_ylabel(\"CCDF\")\n",
    "axs[0].set_yticks([0, 0.5, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    lcc_neg = cc_results[language][1][\"neg\"]\n",
    "    lcc_pos = cc_results[language][1][\"pos\"]\n",
    "    weighted_lcc_neg = cc_results[language][1][\"weighted_neg\"]\n",
    "    weighted_lcc_pos = cc_results[language][1][\"weighted_pos\"]\n",
    "    \n",
    "    print(language, \"lcc\", ks_2samp(lcc_neg, lcc_pos))\n",
    "    print(language, \"weightedlcc\", ks_2samp(weighted_lcc_neg, weighted_lcc_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    pos_words = set()\n",
    "    neg_words = set()\n",
    "    language_df = centrality_results_df[(centrality_results_df[\"language\"] == language) & (centrality_results_df[\"multiplicity_threshold\"] == 1)]\n",
    "    deg_top_words = language_df.sort_values(\"degree\", ascending=False)[\"word\"].head(10).tolist()\n",
    "    bet_top_words = language_df.sort_values(\"betweenness\", ascending=False)[\"word\"].head(10).tolist()\n",
    "    clo_top_words = language_df.sort_values(\"closeness\", ascending=False)[\"word\"].head(10).tolist()\n",
    "    \n",
    "    weighted_deg_top_words = language_df.sort_values(\"weighted_degree\", ascending=False)[\"word\"].head(10).tolist()\n",
    "    weighted_bet_top_words = language_df.sort_values(\"weighted_betweenness\", ascending=False)[\"word\"].head(10).tolist()\n",
    "    weighted_clo_top_words = language_df.sort_values(\"weighted_closeness\", ascending=False)[\"word\"].head(10).tolist()\n",
    "    #translator= Translator(to_lang=\"en\", from_lang=language)\n",
    "    print(\"normal\")\n",
    "    for i in range(0, 10):\n",
    "        #bet_translation = translator.translate(bet_top_words[i])\n",
    "        #clo_translation = translator.translate(clo_top_words[i])\n",
    "        if deg_top_words[i] in sentiment_lexica[language][\"pos\"]:\n",
    "            print(\"\\cellcolor{green!25}\", deg_top_words[i], \"&\", end=\" \")        \n",
    "        elif deg_top_words[i] in sentiment_lexica[language][\"neg\"]:\n",
    "            print(\"\\cellcolor{red!25}\", deg_top_words[i], \"&\", end=\" \")\n",
    "        \n",
    "        if bet_top_words[i] in sentiment_lexica[language][\"pos\"]:\n",
    "            print(\"\\cellcolor{green!25}\", bet_top_words[i], \"&\", end=\" \")        \n",
    "        elif bet_top_words[i] in sentiment_lexica[language][\"neg\"]:\n",
    "            print(\"\\cellcolor{red!25}\", bet_top_words[i], \"&\", end=\" \")\n",
    "            \n",
    "        if clo_top_words[i] in sentiment_lexica[language][\"pos\"]:\n",
    "            print(\"\\cellcolor{green!25}\", clo_top_words[i], \"\\\\\\\\\")        \n",
    "        elif clo_top_words[i] in sentiment_lexica[language][\"neg\"]:\n",
    "            print(\"\\cellcolor{red!25}\", clo_top_words[i], \"\\\\\\\\\")\n",
    "    \n",
    "    print()\n",
    "    print(\"weighted\")\n",
    "    for i in range(0, 10):\n",
    "        #bet_translation = translator.translate(bet_top_words[i])\n",
    "        #clo_translation = translator.translate(clo_top_words[i])\n",
    "        if weighted_deg_top_words[i] in sentiment_lexica[language][\"pos\"]:\n",
    "            print(\"\\cellcolor{green!25}\", weighted_deg_top_words[i], \"&\", end=\" \")        \n",
    "        elif weighted_deg_top_words[i] in sentiment_lexica[language][\"neg\"]:\n",
    "            print(\"\\cellcolor{red!25}\", weighted_deg_top_words[i], \"&\", end=\" \")\n",
    "        \n",
    "        if weighted_bet_top_words[i] in sentiment_lexica[language][\"pos\"]:\n",
    "            print(\"\\cellcolor{green!25}\", weighted_bet_top_words[i], \"&\", end=\" \")        \n",
    "        elif weighted_bet_top_words[i] in sentiment_lexica[language][\"neg\"]:\n",
    "            print(\"\\cellcolor{red!25}\", weighted_bet_top_words[i], \"&\", end=\" \")\n",
    "            \n",
    "        if weighted_clo_top_words[i] in sentiment_lexica[language][\"pos\"]:\n",
    "            print(\"\\cellcolor{green!25}\", weighted_clo_top_words[i], \"\\\\\\\\\")        \n",
    "        elif weighted_clo_top_words[i] in sentiment_lexica[language][\"neg\"]:\n",
    "            print(\"\\cellcolor{red!25}\", weighted_clo_top_words[i], \"\\\\\\\\\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_names_mapping = {\n",
    "    \"degree\": \"Degree\",\n",
    "    \"weighted_degree\": \"Weighted\\nDegree\",\n",
    "    \"betweenness\": \"Betweenness\",\n",
    "    \"weighted_betweenness\": \"Weighted\\nBetweenness\",\n",
    "    \"closeness\": \"Closeness\",\n",
    "    \"weighted_closeness\": \"Weighted\\nCloseness\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, figsize=(15,4), sharey=True)\n",
    "for axidx, centrality in enumerate([\"degree\", \"betweenness\", \"closeness\",  \"weighted_degree\", \"weighted_betweenness\", \"weighted_closeness\"]):\n",
    "    combined_results = []\n",
    "    for language in languages:\n",
    "        language_df = centrality_results_df[(centrality_results_df[\"language\"] == language) & (centrality_results_df[\"multiplicity_threshold\"] == 1)]\n",
    "        temp_centrality_df =  language_df.sort_values(centrality, ascending=False).head(100)[\"word\"].to_frame()\n",
    "        \n",
    "        def get_word_sentiment(word):\n",
    "            if word in sentiment_lexica[language][\"pos\"]:\n",
    "                return \"positive\"\n",
    "            elif word in sentiment_lexica[language][\"neg\"]:\n",
    "                return \"negative\"\n",
    "        \n",
    "        temp_centrality_df[\"sentiment\"] = temp_centrality_df[\"word\"].apply(get_word_sentiment)\n",
    "        temp_centrality_df[\"language\"] = language\n",
    "        combined_results.append(temp_centrality_df)\n",
    "    combined_results_df = pd.concat(combined_results)\n",
    "    props = combined_results_df.groupby(\"language\")[\"sentiment\"].value_counts(normalize=True).unstack()\n",
    "    props[\"total\"] = props[\"negative\"] + props[\"positive\"]\n",
    "    sns.barplot(data=props.reset_index(), x=\"language\", y=\"total\", color=\"lightcoral\", ax=axs[axidx], order=[\"German\", \"French\", \"Italian\", \"Spanish\"])\n",
    "    sns.barplot(data=props.reset_index(), x=\"language\", y=\"positive\", color=\"lightgreen\", ax=axs[axidx], order=[\"German\", \"French\", \"Italian\", \"Spanish\"])\n",
    "    axs[axidx].set_xticklabels(axs[axidx].get_xticklabels(), rotation=90)\n",
    "    axs[axidx].set_title(centrality_names_mapping[centrality])\n",
    "    axs[axidx].set_xlabel(None)\n",
    "    axs[axidx].set_ylabel(None)\n",
    "axs[0].set_ylabel(\"Percentage\")\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "axs[0].set_yticklabels([0, 25, 50, 75, 100])\n",
    "plt.subplots_adjust(bottom=0.27, top=0.82)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_smaller_sig = 0\n",
    "num_larger_sig = 0\n",
    "num_smaller_non_sig = 0 \n",
    "num_larger_non_sig = 0\n",
    "for centrality in [\"degree\", \"betweenness\", \"closeness\",  \"weighted_degree\", \"weighted_betweenness\", \"weighted_closeness\"]:\n",
    "    for language in languages:\n",
    "        language_df = centrality_results_df[(centrality_results_df[\"language\"] == language) & (centrality_results_df[\"multiplicity_threshold\"] == 1)]\n",
    "        temp_centrality_df =  language_df.sort_values(centrality, ascending=False).head(100)[\"word\"].to_frame()\n",
    "        \n",
    "        def get_word_sentiment(word):\n",
    "            if word in sentiment_lexica[language][\"pos\"]:\n",
    "                return \"positive\"\n",
    "            elif word in sentiment_lexica[language][\"neg\"]:\n",
    "                return \"negative\"\n",
    "        \n",
    "        temp_centrality_df[\"sentiment\"] = temp_centrality_df[\"word\"].apply(get_word_sentiment)\n",
    "        neg_count = temp_centrality_df[\"sentiment\"].value_counts()[\"negative\"]\n",
    "        neg_net_ratio = neg_count / 100\n",
    "        neg_graph_ratio = len(np.where(np.array(graphs[language].vs[\"sentiment\"]) == \"neg\")[0]) / graphs[language].vcount()\n",
    "        if neg_net_ratio < neg_graph_ratio:\n",
    "            pvalue = proportions_ztest(count=neg_count, nobs=100, value=neg_graph_ratio, alternative=\"smaller\")[1]\n",
    "            alternative = \"smaller\"\n",
    "        else:\n",
    "            pvalue = proportions_ztest(count=neg_count, nobs=100, value=neg_graph_ratio, alternative=\"larger\")[1]\n",
    "            alternative = \"larger\"\n",
    "            \n",
    "        if pvalue < 0.05:\n",
    "            sig = \"significant\"\n",
    "        else:\n",
    "            sig = \"non-significant\"\n",
    "            \n",
    "        if alternative == \"smaller\" and sig == \"significant\": num_smaller_sig += 1\n",
    "        if alternative == \"larger\" and sig == \"significant\": num_larger_sig += 1\n",
    "        if alternative == \"smaller\" and sig == \"non-significant\": num_smaller_non_sig += 1\n",
    "        if alternative == \"larger\" and sig == \"non-significant\": num_larger_non_sig += 1\n",
    "            \n",
    "        print(centrality, language, neg_net_ratio, round(neg_graph_ratio, 2), alternative, pvalue)\n",
    "        \n",
    "print(num_smaller_sig)\n",
    "print(num_larger_sig)\n",
    "print(num_smaller_non_sig)\n",
    "print(num_larger_non_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    #print(language)\n",
    "    language_df = centrality_results_df[(centrality_results_df[\"language\"] == language) & (centrality_results_df[\"multiplicity_threshold\"] == 1)].set_index(\"word\")\n",
    "    deg = language_df[\"degree\"]\n",
    "    bet = language_df[\"betweenness\"]\n",
    "    clo = language_df[\"closeness\"]\n",
    "    weighted_deg = language_df[\"weighted_degree\"]\n",
    "    weighted_bet = language_df[\"weighted_betweenness\"]\n",
    "    weighted_clo = language_df[\"weighted_closeness\"]\n",
    "    if spearmanr(deg, bet)[1] > 0.05:\n",
    "        print(language, \"degbet\")\n",
    "    if spearmanr(deg, clo)[1] > 0.05:\n",
    "        print(language, \"degclo\")\n",
    "    if spearmanr(bet, clo)[1] > 0.05:\n",
    "        print(language, \"betclo\")\n",
    "    if spearmanr(weighted_deg, weighted_bet)[1] > 0.05:\n",
    "        print(language, \"weighted_degbet\")\n",
    "    if spearmanr(weighted_deg, weighted_clo)[1] > 0.05:\n",
    "        print(language, \"weighted_degclo\")\n",
    "    if spearmanr(weighted_bet, weighted_clo)[1] > 0.05:\n",
    "        print(language, \"weighted_betclo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    language_df = texts_df[texts_df[\"language\"] == language]\n",
    "    print(language_df[\"sentiment\"].mean().round(3))\n",
    "    print(\"++++\")\n",
    "    for mt in multiplicity_thresholds:\n",
    "        print(round(assortativity_results[language][mt][\"degree\"], 3))\n",
    "        print(round(assortativity_results[language][mt][\"sentiment\"], 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    language_df = texts_df[texts_df[\"language\"] == language]\n",
    "    fig, ax = plt.subplots(figsize=(10,2.5))\n",
    "    sns.lineplot(data=language_df, x=\"date\", y=\"sentiment\")\n",
    "    plt.draw()\n",
    "    ax.set_xlabel(\"Years\")\n",
    "    ax.set_ylim(-0.55, 0.55)\n",
    "    ax.set_ylabel(\"Mean\\nSentiment\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    plt.subplots_adjust(bottom=0.42, top=0.96, right=0.97)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = texts_df[\"topics\"].apply(lambda x:pd.Series(list(x))).reset_index().melt(id_vars=\"index\").dropna()[[\"index\", \"value\"]].set_index(\"index\")\n",
    "t_s_df = pd.merge(topics, texts_df[\"sentiment\"].to_frame(), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "sns.barplot(data=t_s_df, x=\"value\", y=\"sentiment\", ax=ax)\n",
    "ax.set_xlabel(\"Topics\")\n",
    "ax.set_ylabel(\"Mean\\nSentiment\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
