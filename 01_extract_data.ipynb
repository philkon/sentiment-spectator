{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from lxml import etree, objectify\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/dispecs-2019-06-17/\"\n",
    "results_dir = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc('ps',fonttype = 42)\n",
    "plt.rc('pdf',fonttype = 42)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.use14corefonts'] = True\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_filenames = glob.glob(\"{}*.xml\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(encoding=\"utf-8\", recover=True, huge_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_nde = {\n",
    "    None: None,\n",
    "    \"E1\": \"Ebene 1\",\n",
    "    \"E2\": \"Ebene 2\",\n",
    "    \"E3\": \"Ebene 3\",\n",
    "    \"E4\": \"Ebene 4\",\n",
    "    \"E5\": \"Ebene 5\",\n",
    "    \"E6\": \"Ebene 6\"\n",
    "}\n",
    "\n",
    "def_ndf = {\n",
    "    None: None,\n",
    "    \"AE\": \"Allgemeine Erzählung\",\n",
    "    \"SP\": \"Selbstportrait\",\n",
    "    \"FP\": \"Fremdportrait\",\n",
    "    \"D\": \"Dialog\",\n",
    "    \"AL\": \"Allegorisches Erzählen\",\n",
    "    \"TR\": \"Traumerzählung\",\n",
    "    \"F\": \"Fabelerzählung\",\n",
    "    \"S\": \"Satirisches Erzählen\",\n",
    "    \"EX\": \"Exemplarisches Erzählen\",\n",
    "    \"UT\": \"Utopische Erzählung\",\n",
    "    \"MT\": \"Metatextualität\",\n",
    "    \"ZM\": \"Zitat/Motto\",\n",
    "    \"LB\": \"Leserbrief\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for errors\n",
    "file_errors = {\n",
    "    \"title_element\": [],\n",
    "    \"title_none\": [],\n",
    "    \"volume_issues_element\": [],\n",
    "    \"volume_none\": [],\n",
    "    \"issue_none\": [],\n",
    "    \"journal_title_element\": [],\n",
    "    \"journal_title_none\": [],\n",
    "    \"author_element\": [],\n",
    "    \"author_none\": [],\n",
    "    \"author_whitespace\": [],\n",
    "    \"country_element\": [],\n",
    "    \"country_none\": [],\n",
    "    \"language_element\": [],\n",
    "    \"language_none\": [],\n",
    "    \"date_element\": [],\n",
    "    \"date_none\": [],\n",
    "    \"topics_none\": [],\n",
    "    \"text_element\": []\n",
    "}\n",
    "\n",
    "# temp results list\n",
    "texts_list = []\n",
    "\n",
    "# parse xml files\n",
    "for fn in tqdm(xml_filenames):\n",
    "    filename = fn.split(\"/\")[-1]\n",
    "    #print(filename)\n",
    "    #if fn != \"data/dispecs-2019-06-03/mws.3424.xml\":continue\n",
    "    \n",
    "    et = etree.parse(fn, parser)\n",
    "    \n",
    "    # title\n",
    "    try:\n",
    "        title = et.xpath(\"//tei:titleStmt/tei:title\", namespaces=namespaces)[0].text\n",
    "        if title is None:\n",
    "            title = \"missing\"\n",
    "            file_errors[\"title_none\"].append(filename)\n",
    "    except IndexError:\n",
    "        title = \"missing\"\n",
    "        file_errors[\"title_element\"].append(filename)\n",
    "        \n",
    "    # volume & issue\n",
    "    try:\n",
    "        volume, issue = et.xpath(\"//tei:sourceDesc/tei:bibl/tei:biblScope \", namespaces=namespaces)\n",
    "        volume = volume.text\n",
    "        issue = issue.text\n",
    "        if volume is None:\n",
    "            volume = \"missing\"\n",
    "            file_errors[\"volume_none\"].append(filename)\n",
    "        if issue is None:\n",
    "            issue = \"missing\"\n",
    "            file_errors[\"issue_none\"].append(filename)    \n",
    "    except ValueError:\n",
    "        volume = \"missing\"\n",
    "        issue = \"missing\"\n",
    "        file_errors[\"volume_issues_element\"].append(filename)\n",
    "        \n",
    "    #journal title\n",
    "    try:\n",
    "        journal_title = et.xpath(\"//tei:sourceDesc/tei:bibl/tei:title \", namespaces=namespaces)[0].text.replace(\"\\n\", \" \").strip()\n",
    "        journal_title = re.sub(\" +\", \" \", journal_title)\n",
    "        if journal_title is None:\n",
    "            file_errors[\"journal_title_none\"].append(filename)\n",
    "    except IndexError:\n",
    "        journal_title = \"missing\"\n",
    "        file_errors[\"journal_title_element\"].append(filename)\n",
    "    \n",
    "    # author\n",
    "    try:\n",
    "        author = et.xpath(\"//tei:titleStmt/tei:author\", namespaces=namespaces)[0].text        \n",
    "        if author.endswith(\" \") or author.startswith(\" \"):\n",
    "            file_errors[\"author_whitespace\"].append(filename)\n",
    "            author = author.strip()\n",
    "        author = re.sub(\"\\s+\", \" \", author)\n",
    "        if author == \"Anónimo\" or author == \"Anonymus\":\n",
    "            author = \"Anonym\"\n",
    "        if author is None:\n",
    "            author = \"missing\"\n",
    "            file_errors[\"author_none\"].append(filename)\n",
    "    except:\n",
    "        author = \"missing\"\n",
    "        file_errors[\"author_element\"].append(filename)\n",
    "    \n",
    "    # country\n",
    "    try:\n",
    "        country = et.xpath(\"//tei:sourceDesc/tei:bibl/tei:placeName\", namespaces=namespaces)[0].text\n",
    "        if country is None:\n",
    "            country = \"missing\"\n",
    "            file_errors[\"country_none\"].append(filename)\n",
    "    except IndexError:\n",
    "        country = \"missing\"\n",
    "        file_errors[\"country_element\"].append(filename)\n",
    "    \n",
    "    # language\n",
    "    try:\n",
    "        language = et.xpath(\"//tei:profileDesc/tei:langUsage/tei:language\", namespaces=namespaces)[0].text\n",
    "        if language is None:\n",
    "            language = \"missing\"\n",
    "            file_errors[\"language_none\"].append(filename)\n",
    "    except IndexError:\n",
    "        language = \"missing\"\n",
    "        file_errors[\"language_element\"].append(filename)\n",
    "    \n",
    "    # date\n",
    "    try:\n",
    "        date = et.xpath(\"//tei:sourceDesc/tei:bibl/tei:date\", namespaces=namespaces)[0].text\n",
    "        if date is None:\n",
    "            date = \"missing\"\n",
    "            file_errors[\"date_none\"].append(filename)\n",
    "    except IndexError:\n",
    "        date = \"missing\"\n",
    "        file_errors[\"date_element\"].append(filename)\n",
    "    \n",
    "    # topics\n",
    "    topics = set()\n",
    "    for t in et.xpath(\"//tei:profileDesc/tei:textClass/tei:keywords/tei:term/tei:term[@xml:lang='en']\", namespaces=namespaces):\n",
    "        topics.add(re.sub(\"\\s+\", \" \", t.text).strip())\n",
    "    if len(topics) == 0:\n",
    "        file_errors[\"topics_none\"].append(filename)\n",
    "    \n",
    "    basic_results = {\n",
    "        \"filename\": filename,\n",
    "        \"title\": title,\n",
    "        \"volume\": volume,\n",
    "        \"issue\": issue,\n",
    "        \"author\": author,\n",
    "        \"country\": country,\n",
    "        \"language\": language,\n",
    "        \"date\": date,\n",
    "        \"topics\": topics,\n",
    "    }\n",
    "    \n",
    "    # text\n",
    "    index = pd.MultiIndex.from_product([def_nde.keys(), def_ndf.keys()],  names=[\"nde\", \"ndf\"])\n",
    "    temp_text_df = pd.DataFrame(index=index, columns=[\"text\", \"places\", \"persons\"])\n",
    "    temp_text_df[\"text\"] = \"\"\n",
    "    temp_text_df[\"places\"] = [set() for i in range(len(temp_text_df))]\n",
    "    temp_text_df[\"persons\"] = [set() for i in range(len(temp_text_df))]\n",
    "    temp_text_df[\"works\"] = [set() for i in range(len(temp_text_df))]\n",
    "    temp_text_df[\"filename\"] = filename\n",
    "    temp_text_df[\"title\"] = re.sub(\"\\s+\", \" \", title).strip()\n",
    "    temp_text_df[\"volume\"] = re.sub(\"\\s+\", \" \", volume).strip()\n",
    "    temp_text_df[\"issue\"] = re.sub(\"\\s+\", \" \", issue).strip()\n",
    "    temp_text_df[\"journal_title\"] = re.sub(\"\\s+\", \" \", journal_title).strip()\n",
    "    temp_text_df[\"author\"] = re.sub(\"\\s+\", \" \", author).strip()\n",
    "    temp_text_df[\"country\"] = re.sub(\"\\s+\", \" \", country).strip()\n",
    "    temp_text_df[\"language\"] = re.sub(\"\\s+\", \" \", language).strip()\n",
    "    temp_text_df[\"date\"] = re.sub(\"\\s+\", \" \", date).strip()\n",
    "    temp_text_df[\"topics\"] = [topics for i in range(len(temp_text_df))]\n",
    "            \n",
    "    try:\n",
    "        text_element = et.xpath(\"//tei:text[@ana='layout']/tei:body\", namespaces=namespaces)[0]\n",
    "    except IndexError:\n",
    "        file_errors[\"text_element\"].append(filename)\n",
    "        continue\n",
    "    \n",
    "    cur_ndes = [None]\n",
    "    cur_ndfs = [None]\n",
    "    for c in text_element.iterdescendants():\n",
    "        tag = c.tag.replace(\"{http://www.tei-c.org/ns/1.0}\", \"\")\n",
    "        #print(tag)\n",
    "        if tag == \"milestone\":\n",
    "            kind = c.attrib[\"unit\"]\n",
    "            if kind in def_nde:\n",
    "                try:\n",
    "                    c.attrib[\"rend\"]\n",
    "                    if len(cur_ndes) > 1: cur_ndes.pop()\n",
    "                except KeyError:\n",
    "                    cur_ndes.append(kind)\n",
    "            elif kind in def_ndf:\n",
    "                try:\n",
    "                    c.attrib[\"rend\"]\n",
    "                    if len(cur_ndfs) > 1: cur_ndfs.pop()\n",
    "                except KeyError:\n",
    "                    cur_ndfs.append(kind)\n",
    "            #print(cur_ndes, cur_ndf)\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.tail\n",
    "            except (TypeError, KeyError):\n",
    "                pass\n",
    "        if tag == \"head\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.text\n",
    "            except TypeError:\n",
    "                pass\n",
    "        if tag == \"p\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.text\n",
    "            except TypeError:\n",
    "                pass\n",
    "        if tag == \"placeName\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"places\"].add(re.sub(\"\\s+\", \" \", c.text).strip())\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.text\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.tail\n",
    "            except TypeError:\n",
    "                pass\n",
    "        if tag == \"persName\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"persons\"].add(re.sub(\"\\s+\", \" \", c.text).strip())\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.text\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.tail\n",
    "            except TypeError:\n",
    "                pass\n",
    "        if tag == \"name\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"works\"].add(re.sub(\"\\s+\", \" \", c.text).strip())\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.text\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.tail\n",
    "            except TypeError:\n",
    "                pass\n",
    "        if tag == \"hi\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.text\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" + c.tail                 \n",
    "            except (TypeError, IndexError):\n",
    "                pass\n",
    "        if tag == \"lb\" or tag == \"pb\":\n",
    "            try:\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" +  c.text\n",
    "                temp_text_df.loc[(cur_ndes[-1], cur_ndfs[-1]), \"text\"] += \" \" +  c.tail\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # prepare data\n",
    "    temp_text_df = temp_text_df[temp_text_df[\"text\"].apply(lambda x: len(x.strip()) > 0)].reset_index()\n",
    "    temp_text_df[\"text\"] = temp_text_df[\"text\"].apply(lambda x: re.sub(\"\\s+\", \" \", x).strip())\n",
    "    texts_list.append(temp_text_df)\n",
    "    \n",
    "texts_df = pd.concat(texts_list, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.loc[7645, \"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.to_pickle(\"data/processed/texts.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot file errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_results = pd.Series({key:len(value) for (key, value) in file_errors.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_results.plot(kind=\"bar\", figsize=[10,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save errors to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(file_errors).apply(lambda x:\", \".join(x)).to_csv(\"{}file_errors.csv\".format(results_dir), sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
